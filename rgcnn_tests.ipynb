{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/victor/workspace/thesis_ws/datasets/Modelnet10\n",
      "Train dataset shape: ModelNet10(3991)\n",
      "Test dataset shape:  ModelNet10(908)\n",
      "Data(pos=[1024, 3], y=[1], normal=[1024, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.transforms import SamplePoints\n",
    "from torch_geometric.transforms import NormalizeScale\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import torch_geometric.nn.conv as conv\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "\n",
    "num_points = 1024\n",
    "batch_size = 16\n",
    "modelnet_num = 10\n",
    "\n",
    "transforms = Compose([SamplePoints(num_points, include_normals=True), NormalizeScale()])\n",
    "\n",
    "root = \"/home/victor/workspace/thesis_ws/datasets/Modelnet\" + str(modelnet_num)\n",
    "print(root)\n",
    "dataset_train = ModelNet(root=root, name=str(modelnet_num), train=True, transform=transforms)\n",
    "dataset_test = ModelNet(root=root, name=str(modelnet_num), train=False, transform=transforms)\n",
    "\n",
    "# Verification...\n",
    "print(f\"Train dataset shape: {dataset_train}\")\n",
    "print(f\"Test dataset shape:  {dataset_test}\")\n",
    "\n",
    "print(dataset_train[0])\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "dataloader_test  = DataLoader(dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GetGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates the weighted adjacency matrix 'W'\n",
    "        Taked directly from RGCNN\n",
    "        \"\"\"\n",
    "\n",
    "        super(GetGraph, self).__init__()\n",
    "\n",
    "    def forward(self, point_cloud, batch):\n",
    "        point_cloud = point_cloud.reshape(batch_size, -1, 6)\n",
    "        point_cloud_transpose = point_cloud.permute(0, 2, 1)\n",
    "        point_cloud_inner = torch.matmul(point_cloud, point_cloud_transpose)\n",
    "        point_cloud_inner = -2 * point_cloud_inner\n",
    "        point_cloud_square = torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True)\n",
    "        point_cloud_square_tranpose = point_cloud_square.permute(0, 2, 1)\n",
    "        adj_matrix = point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "        adj_matrix = torch.exp(-adj_matrix)\n",
    "        edge_index, edge_weight = utils.dense_to_sparse(adj_matrix)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "def get_graph(point_cloud, batch):\n",
    "    point_cloud = point_cloud.reshape(batch.unique().shape[0], -1, 6)\n",
    "    point_cloud_transpose = point_cloud.permute(0, 2, 1)\n",
    "    point_cloud_inner = torch.matmul(point_cloud, point_cloud_transpose)\n",
    "    point_cloud_inner = -2 * point_cloud_inner\n",
    "    point_cloud_square = torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True)\n",
    "    point_cloud_square_tranpose = point_cloud_square.permute(0, 2, 1)\n",
    "    adj_matrix = point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "    adj_matrix = torch.exp(-adj_matrix)\n",
    "    edge_index, edge_weight = utils.dense_to_sparse(adj_matrix)\n",
    "\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def get_graph_v2(point_cloud, batch):\n",
    "    point_cloud = point_cloud.reshape(batch.unique().shape[0], -1, 6)\n",
    "    adj_matrix = torch.exp(-(torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True) - 2 * torch.matmul(point_cloud, point_cloud.permute(0, 2, 1)) + torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True).permute(0, 2, 1)))\n",
    "\n",
    "    return utils.dense_to_sparse(adj_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if False:\n",
    "    trainiter = iter(dataloader_train)\n",
    "    data = trainiter.next()\n",
    "    point_cloud = torch.cat([data.pos, data.normal], axis=1)\n",
    "    point_cloud = point_cloud.to(\"cuda\")\n",
    "    batch = data.batch.to(\"cuda\") \n",
    "    #print(torch.cuda.memory_allocated(device=\"cuda\"))\n",
    "    graph1 = get_graph(point_cloud, batch)\n",
    "    #print(torch.cuda.memory_allocated(device=\"cuda\"))\n",
    "    del graph1\n",
    "    graph2 = get_graph(point_cloud, batch)\n",
    "    #print(torch.cuda.memory_allocated(device=\"cuda\"))\n",
    "    del graph2\n",
    "\n",
    "    del point_cloud, batch\n",
    "    #print(torch.cuda.memory_allocated(device=\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCNN_model(\n",
      "  (conv1): ChebConv(6, 128, K=6, normalization=sym)\n",
      "  (fc1): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn.conv import ChebConv\n",
    "from torch.nn import Linear\n",
    "from torch_cluster import knn_graph\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "class RGCNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RGCNN_model, self).__init__()\n",
    "        self.conv1  = ChebConv(6, 128, 6)\n",
    "        self.fc1    = Linear(128, modelnet_num)\n",
    "        self.relu   = nn.ReLU()\n",
    "        #self.get_graph = GetGraph()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        edge_index, edge_weight = get_graph(x, batch=batch)\n",
    "        out = self.conv1(x=x, edge_index=edge_index, edge_weight=edge_weight, batch=batch)\n",
    "        out = self.relu(out)\n",
    "        out = global_max_pool(out, batch)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "model = RGCNN_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.7230, Test Accuracy: 0.7808\n",
      "Epoch: 02, Loss: 0.4011, Test Accuracy: 0.8018\n",
      "Epoch: 03, Loss: 0.3330, Test Accuracy: 0.8304\n",
      "Epoch: 04, Loss: 0.3116, Test Accuracy: 0.7952\n",
      "Epoch: 05, Loss: 0.2842, Test Accuracy: 0.8469\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.cat([data.pos, data.normal], dim=1)\n",
    "        logits  = model(x.to(device),  data.batch.to(device))\n",
    "        loss    = criterion(logits, data.y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    for data in loader:\n",
    "        x = torch.cat([data.pos, data.normal], dim=1)\n",
    "        logits = model(x.to(device), data.batch.to(device))\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y.to(device)).sum())\n",
    "\n",
    "    return total_correct / len(loader.dataset)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(model, optimizer, dataloader_train)\n",
    "    test_acc = test(model, dataloader_test)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Test Accuracy: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5645a68e3fb500d83b7487a2b0474374adaf02ad9fa0e0c4fb97f8a41dacfa89"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('thesis_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
