{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4effe66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "#sys.path.append(BASE_DIR)\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    \"\"\" Shuffle data and labels.\n",
    "        Input:\n",
    "          data: B,N,... numpy array\n",
    "          label: B,... numpy array\n",
    "        Return:\n",
    "          shuffled data, label and shuffle indices\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(labels))\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx, ...], labels[idx], idx\n",
    "\n",
    "def shuffle_points(batch_data):\n",
    "    \"\"\" Shuffle orders of points in each point cloud -- changes FPS behavior.\n",
    "        Use the same shuffling idx for the entire batch.\n",
    "        Input:\n",
    "            BxNxC array\n",
    "        Output:\n",
    "            BxNxC array\n",
    "    \"\"\"\n",
    "    idx = np.arange(batch_data.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    return batch_data[:,idx,:]\n",
    "\n",
    "def rotate_point_cloud(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "def rotate_point_cloud_z(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, sinval, 0],\n",
    "                                    [-sinval, cosval, 0],\n",
    "                                    [0, 0, 1]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "def rotate_point_cloud_with_normal(batch_xyz_normal):\n",
    "    ''' Randomly rotate XYZ, normal point cloud.\n",
    "        Input:\n",
    "            batch_xyz_normal: B,N,6, first three channels are XYZ, last 3 all normal\n",
    "        Output:\n",
    "            B,N,6, rotated XYZ, normal point cloud\n",
    "    '''\n",
    "    for k in range(batch_xyz_normal.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_xyz_normal[k,:,0:3]\n",
    "        shape_normal = batch_xyz_normal[k,:,3:6]\n",
    "        batch_xyz_normal[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "        batch_xyz_normal[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), rotation_matrix)\n",
    "    return batch_xyz_normal\n",
    "\n",
    "def rotate_perturbation_point_cloud_with_normal(batch_data, angle_sigma=0.06, angle_clip=0.18):\n",
    "    \"\"\" Randomly perturb the point clouds by small rotations\n",
    "        Input:\n",
    "          BxNx6 array, original batch of point clouds and point normals\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n",
    "        Rx = np.array([[1,0,0],\n",
    "                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n",
    "                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n",
    "        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n",
    "                       [0,1,0],\n",
    "                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n",
    "        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n",
    "                       [np.sin(angles[2]),np.cos(angles[2]),0],\n",
    "                       [0,0,1]])\n",
    "        R = np.dot(Rz, np.dot(Ry,Rx))\n",
    "        shape_pc = batch_data[k,:,0:3]\n",
    "        shape_normal = batch_data[k,:,3:6]\n",
    "        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), R)\n",
    "        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), R)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def rotate_point_cloud_by_angle(batch_data, rotation_angle):\n",
    "    \"\"\" Rotate the point cloud along up direction with certain angle.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        #rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k,:,0:3]\n",
    "        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "def rotate_point_cloud_by_angle_with_normal(batch_data, rotation_angle):\n",
    "    \"\"\" Rotate the point cloud along up direction with certain angle.\n",
    "        Input:\n",
    "          BxNx6 array, original batch of point clouds with normal\n",
    "          scalar, angle of rotation\n",
    "        Return:\n",
    "          BxNx6 array, rotated batch of point clouds iwth normal\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        #rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k,:,0:3]\n",
    "        shape_normal = batch_data[k,:,3:6]\n",
    "        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1,3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "\n",
    "def rotate_perturbation_point_cloud(batch_data, angle_sigma=0.06, angle_clip=0.18):\n",
    "    \"\"\" Randomly perturb the point clouds by small rotations\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n",
    "        Rx = np.array([[1,0,0],\n",
    "                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n",
    "                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n",
    "        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n",
    "                       [0,1,0],\n",
    "                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n",
    "        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n",
    "                       [np.sin(angles[2]),np.cos(angles[2]),0],\n",
    "                       [0,0,1]])\n",
    "        R = np.dot(Rz, np.dot(Ry,Rx))\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, jittered batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert(clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data\n",
    "\n",
    "def shift_point_cloud(batch_data, shift_range=0.1):\n",
    "    \"\"\" Randomly shift point cloud. Shift is per point cloud.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, shifted batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    shifts = np.random.uniform(-shift_range, shift_range, (B,3))\n",
    "    for batch_index in range(B):\n",
    "        batch_data[batch_index,:,:] += shifts[batch_index,:]\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):\n",
    "    \"\"\" Randomly scale the point cloud. Scale is per point cloud.\n",
    "        Input:\n",
    "            BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "            BxNx3 array, scaled batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    scales = np.random.uniform(scale_low, scale_high, B)\n",
    "    for batch_index in range(B):\n",
    "        batch_data[batch_index,:,:] *= scales[batch_index]\n",
    "    return batch_data\n",
    "\n",
    "def random_point_dropout(batch_pc, max_dropout_ratio=0.875):\n",
    "    ''' batch_pc: BxNx3 '''\n",
    "    for b in range(batch_pc.shape[0]):\n",
    "        dropout_ratio =  np.random.random()*max_dropout_ratio # 0~0.875\n",
    "        drop_idx = np.where(np.random.random((batch_pc.shape[1]))<=dropout_ratio)[0]\n",
    "        if len(drop_idx)>0:\n",
    "            batch_pc[b,drop_idx,:] = batch_pc[b,0,:] # set to the first point\n",
    "    return batch_pc\n",
    "\n",
    "\n",
    "def getDataFiles(list_filename):\n",
    "    return [line.rstrip() for line in open(list_filename)]\n",
    "\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)\n",
    "\n",
    "def loadDataFile(filename):\n",
    "    return load_h5(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb56d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2468\n",
      "0.650337934494\n",
      "((1024, 3), <type 'numpy.ndarray'>, array([0], dtype=int32))\n",
      "True\n",
      "(32, 1024, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('/home/victor/workspace/thesis_ws/RGCNN/'))\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "class ModelNetDataset():\n",
    "    def __init__(self, root, batch_size = 32, npoints = 1024, split='train', normalize=True, normal_channel=False, modelnet10=False, cache_size=15000, shuffle=None):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.npoints = npoints\n",
    "        self.normalize = normalize\n",
    "        if modelnet10:\n",
    "            self.catfile = os.path.join(self.root, 'modelnet10_shape_names.txt')\n",
    "        else:\n",
    "            self.catfile = os.path.join(self.root, 'modelnet40_shape_names.txt')\n",
    "        self.cat = [line.rstrip() for line in open(self.catfile)]\n",
    "        self.classes = dict(zip(self.cat, range(len(self.cat))))  \n",
    "        self.normal_channel = normal_channel\n",
    "        \n",
    "        shape_ids = {}\n",
    "        if modelnet10:\n",
    "            shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet10_train.txt'))] \n",
    "            shape_ids['test']= [line.rstrip() for line in open(os.path.join(self.root, 'modelnet10_test.txt'))]\n",
    "        else:\n",
    "            shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_train.txt'))] \n",
    "            shape_ids['test']= [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_test.txt'))]\n",
    "        assert(split=='train' or split=='test')\n",
    "        shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]\n",
    "        # list of (shape_name, shape_txt_file_path) tuple\n",
    "        self.datapath = [(shape_names[i], os.path.join(self.root, shape_names[i], shape_ids[split][i])+'.txt') for i in range(len(shape_ids[split]))]\n",
    "\n",
    "        self.cache_size = cache_size # how many data points to cache in memory\n",
    "        self.cache = {} # from index to (point_set, cls) tuple\n",
    "\n",
    "        if shuffle is None:\n",
    "            if split == 'train': self.shuffle = True\n",
    "            else: self.shuffle = False\n",
    "        else:\n",
    "            self.shuffle = shuffle\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _augment_batch_data(self, batch_data):\n",
    "        if self.normal_channel:\n",
    "            rotated_data = rotate_point_cloud_with_normal(batch_data)\n",
    "            rotated_data = rotate_perturbation_point_cloud_with_normal(rotated_data)\n",
    "        else:\n",
    "            rotated_data = rotate_point_cloud(batch_data)\n",
    "            rotated_data = rotate_perturbation_point_cloud(rotated_data)\n",
    "    \n",
    "        jittered_data = random_scale_point_cloud(rotated_data[:,:,0:3])\n",
    "        jittered_data = shift_point_cloud(jittered_data)\n",
    "        jittered_data = jitter_point_cloud(jittered_data)\n",
    "        rotated_data[:,:,0:3] = jittered_data\n",
    "        return shuffle_points(rotated_data)\n",
    "\n",
    "\n",
    "    def _get_item(self, index): \n",
    "        if index in self.cache:\n",
    "            point_set, cls = self.cache[index]\n",
    "        else:\n",
    "            fn = self.datapath[index]\n",
    "            cls = self.classes[self.datapath[index][0]]\n",
    "            cls = np.array([cls]).astype(np.int32)\n",
    "            point_set = np.loadtxt(fn[1],delimiter=',').astype(np.float32)\n",
    "            # Take the first npoints\n",
    "            point_set = point_set[0:self.npoints,:]\n",
    "            if self.normalize:\n",
    "                point_set[:,0:3] = pc_normalize(point_set[:,0:3])\n",
    "            if not self.normal_channel:\n",
    "                point_set = point_set[:,0:3]\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[index] = (point_set, cls)\n",
    "        return point_set, cls\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self._get_item(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def num_channel(self):\n",
    "        if self.normal_channel:\n",
    "            return 6\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.idxs = np.arange(0, len(self.datapath))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.num_batches = (len(self.datapath)+self.batch_size-1) // self.batch_size\n",
    "        self.batch_idx = 0\n",
    "\n",
    "    def has_next_batch(self):\n",
    "        return self.batch_idx < self.num_batches\n",
    "\n",
    "    def next_batch(self, augment=False):\n",
    "        ''' returned dimension may be smaller than self.batch_size '''\n",
    "        start_idx = self.batch_idx * self.batch_size\n",
    "        end_idx = min((self.batch_idx+1) * self.batch_size, len(self.datapath))\n",
    "        bsize = end_idx - start_idx\n",
    "        batch_data = np.zeros((bsize, self.npoints, self.num_channel()))\n",
    "        batch_label = np.zeros((bsize), dtype=np.int32)\n",
    "        for i in range(bsize):\n",
    "            ps,cls = self._get_item(self.idxs[i+start_idx])\n",
    "            batch_data[i] = ps\n",
    "            batch_label[i] = cls\n",
    "        self.batch_idx += 1\n",
    "        if augment: batch_data = self._augment_batch_data(batch_data)\n",
    "        return batch_data, batch_label\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    d = ModelNetDataset(root = '/home/victor/workspace/thesis_ws/data/modelnet40_normal_resampled', split='test')\n",
    "    print(d.shuffle)\n",
    "    print(len(d))\n",
    "    import time\n",
    "    tic = time.time()\n",
    "    for i in range(10):\n",
    "        ps, cls = d[i]\n",
    "    print(time.time() - tic)\n",
    "    print(ps.shape, type(ps), cls)\n",
    "\n",
    "    print(d.has_next_batch())\n",
    "    ps_batch, cls_batch = d.next_batch(True)\n",
    "    print(ps_batch.shape)\n",
    "    print(cls_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235529bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/victor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/victor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/victor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/victor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/victor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import mymodel, mymodel_knn, seg_model\n",
    "\n",
    "import seg_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def genData(cls, limit=None):\n",
    "    '''\n",
    "    cls = the name of the class - string\n",
    "\n",
    "    '''\n",
    "    assert type(cls) is str # makes sure cls is string xDD\n",
    "\n",
    "    seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n",
    "                   'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46],\n",
    "                   'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27],\n",
    "                   'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15],\n",
    "                   'Knife': [22, 23]}\n",
    "\n",
    "    data = np.load( \"/home/victor/workspace/thesis_ws/RGCNN/cls_data_%s.npy\" % cls)\n",
    "    label = np.load(\"/home/victor/workspace/thesis_ws/RGCNN/cls_label_%s.npy\" % cls)\n",
    "\n",
    "    data = data[:limit]\n",
    "    label = label[:limit]\n",
    "\n",
    "    seg = {}\n",
    "    name = {}\n",
    "    i = 0\n",
    "    for k,v in sorted(seg_classes.items()):\n",
    "        for value in v:\n",
    "            seg[value] = i\n",
    "            name[value] = k\n",
    "        i += 1\n",
    "    cnt = data.shape[0]\n",
    "    cat = np.zeros((cnt))\n",
    "    for i in range(cnt):\n",
    "        #print(label)\n",
    "        cat[i] = seg[label[i]]\n",
    "    return data,label,cat\n",
    "\n",
    "train_data, train_label, train_cat = genData('train')\n",
    "test_data, test_label, test_cat = genData('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c84713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cls_model\n",
    "def train():\n",
    "    '''\n",
    "    train_data, train_label, train_cat = genData('train')\n",
    "    val_data, val_label, val_cat = genData('val')\n",
    "    test_data, test_label, test_cat = genData('test')\n",
    "    '''\n",
    "    train_data, train_label, train_cat = genData('train')\n",
    "    test_data, test_label, test_cat = genData('test')\n",
    "    val_data  = test_data[1467:-1,:,:]\n",
    "    val_label = test_label[1467:-1]\n",
    "    val_cat   = test_cat[1467:-1]\n",
    "    test_data = test_data[0:1467,:,:]\n",
    "    test_label = test_label[0:1467]\n",
    "    test_cat   = test_cat[0:1467]\n",
    "    print(train_data.shape)\n",
    "\n",
    "    params = dict()\n",
    "    params['dir_name'] = 'model'\n",
    "    params['num_epochs'] = 50\n",
    "    params['batch_size'] = 26\n",
    "    params['eval_frequency'] = 30\n",
    "\n",
    "    # Building blocks.\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    params['brelu'] = 'b1relu'\n",
    "    params['pool'] = 'mpool1'\n",
    "\n",
    "    # Number of classes.\n",
    "    # C = y.max() + 1\n",
    "    # assert C == np.unique(y) .size\n",
    "\n",
    "    # Architecture.\n",
    "    params['F'] = [128, 512, 1024]  # Number of graph convolutional filters.\n",
    "    params['K'] = [6, 5, 3]  # Polynomial orders.\n",
    "    params['M'] = [512, 128, 40]  # Output dimensionality of fully connected layers. For classification only\n",
    "\n",
    "    # Optimization.\n",
    "    params['regularization'] = 1e-9\n",
    "    params['dropout'] = 1\n",
    "    params['learning_rate'] = 1e-3\n",
    "    params['decay_rate'] = 0.95\n",
    "    params['momentum'] = 0\n",
    "    params['decay_steps'] = train_data.shape[0] / params['batch_size']\n",
    "\n",
    "    model = cls_model.rgcnn(1024, **params)\n",
    "    accuracy, loss, t_step = model.fit(train_data, train_cat, train_label, val_data, val_cat, val_label,\n",
    "                                       is_continue=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4b4e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9843, 1024, 6)\n",
      "NN architecture\n",
      "  input: M_0 = 1024\n",
      "  layer 1: gconv1\n",
      "    representation: M_0 * F_1= 1024 * 128 = 131072\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 128 * 6 = 768\n",
      "    biases: F_1 = 128\n",
      "  layer 2: gconv2\n",
      "    representation: M_1 * F_2= 1024 * 512 = 524288\n",
      "    weights: F_1 * F_2 * K_2 = 128 * 512 * 5 = 327680\n",
      "    biases: F_2 = 512\n",
      "  layer 3: gconv3\n",
      "    representation: M_2 * F_3= 1024 * 1024 = 1048576\n",
      "    weights: F_2 * F_3 * K_3 = 512 * 1024 * 3 = 1572864\n",
      "    biases: F_3 = 1024\n",
      "  layer 4: fc1\n",
      "    representation: M_4 = 512\n",
      "    weights: M_3 * M_4 = 1048576 * 512 = 536870912\n",
      "    biases: M_4 = 512\n",
      "  layer 5: fc2\n",
      "    representation: M_5 = 128\n",
      "    weights: M_4 * M_5 = 512 * 128 = 65536\n",
      "    biases: M_5 = 128\n",
      "  layer 6: fc3\n",
      "    representation: M_6 = 40\n",
      "    weights: M_5 * M_6 = 128 * 40 = 5120\n",
      "    biases: M_6 = 40\n",
      "Total flops102501777921\n",
      "(26, 40)\n",
      "(26, 1024)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-92266d401ef8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decay_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     accuracy, loss, t_step = model.fit(train_data, train_cat, train_label, val_data, val_cat, val_label,\n\u001b[1;32m     48\u001b[0m                                        is_continue=False)\n",
      "\u001b[0;32m~/workspace/thesis_ws/RGCNN/cls_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertice, F, K, M, filter, brelu, pool, num_epochs, learning_rate, decay_rate, decay_steps, momentum, regularization, dropout, batch_size, eval_frequency, dir_name)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;31m# Build the computational graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchebyshev5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/thesis_ws/RGCNN/cls_model.py\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self, M_0)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mph_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_loss_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mph_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             self.op_train = self.training(self.op_loss, self.learning_rate,\n\u001b[1;32m    217\u001b[0m                                           self.decay_steps, self.decay_rate, self.momentum)\n",
      "\u001b[0;32m~/workspace/thesis_ws/RGCNN/cls_model.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, logits, labels, regularization)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_int64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'regularization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   1873\u001b[0m       raise ValueError(\"Rank mismatch: Rank of labels (received %s) should \"\n\u001b[1;32m   1874\u001b[0m                        \u001b[0;34m\"equal rank of logits minus 1 (received %s).\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m                        (labels_static_shape.ndims, logits.get_shape().ndims))\n\u001b[0m\u001b[1;32m   1876\u001b[0m     \u001b[0;31m# Check if no reshapes are required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2)."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a90c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39606b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ffde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    test_data, test_label, test_cat = genData('test')\n",
    "    params = dict()\n",
    "    params['dir_name'] = 'model'\n",
    "    params['num_epochs'] = 50\n",
    "    params['batch_size'] = 26\n",
    "    params['eval_frequency'] = 30\n",
    "\n",
    "    # Building blocks.\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    params['brelu'] = 'b1relu'\n",
    "    params['pool'] = 'apool1'\n",
    "\n",
    "    # Number of classes.\n",
    "    # C = y.max() + 1\n",
    "    # assert C == np.unique(y) .size\n",
    "\n",
    "    # Architecture.\n",
    "    params['F'] = [128, 512, 1024]  # Number of graph convolutional filters.\n",
    "    params['K'] = [6, 5, 3]  # Polynomial orders.\n",
    "    params['M'] = [512, 128, 10]  # Output dimensionality of fully connected layers. For classification only\n",
    "\n",
    "    # Optimization.\n",
    "    params['regularization'] = 1e-9\n",
    "    params['dropout'] = 1\n",
    "    params['learning_rate'] = 1e-3\n",
    "    params['decay_rate'] = 0.95\n",
    "    params['momentum'] = 0\n",
    "    params['decay_steps'] = test_data.shape[0] / params['batch_size']\n",
    "\n",
    "    model = cls_model.rgcnn(1024, **params)\n",
    "    model.evaluate(test_data, test_cat, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766a9dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (9843, 1024, 6))\n",
      "('Test:  ', (1467, 1024, 6))\n",
      "('Val    ', (1000, 1024, 6))\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = genData('train')\n",
    "test_data, test_label = genData('test')\n",
    "val_data  = test_data[1467:-1,:,:]\n",
    "val_label = test_label[1467:-1]\n",
    "test_data = test_data[0:1467,:,:]\n",
    "test_label = test_label[0:1467]\n",
    "\n",
    "print(\"Train: \", train_data.shape)\n",
    "print(\"Test:  \", test_data.shape)\n",
    "print(\"Val    \", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b3101c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 1)\n",
      "(10000000, 1)\n"
     ]
    }
   ],
   "source": [
    "test = np.load(\"/home/victor/workspace/thesis_ws/RGCNN/temp_data/label_train.npy\")\n",
    "test2 = np.load(\"/home/victor/workspace/thesis_ws/RGCNN/temp_data/label_val.npy\")\n",
    "print(test.shape)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528d5837",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-29512d608e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a92e5b48abcf>\u001b[0m in \u001b[0;36mgenData\u001b[0;34m(cls, limit)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mcat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;31m#,cat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "train_data, train_label = genData('train')\n",
    "test_data, test_label = genData('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
