{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fa46ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  9 14:15:08 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   35C    P8    31W / 340W |     93MiB / 10008MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1284      G   /usr/lib/xorg/Xorg                 39MiB |\r\n",
      "|    0   N/A  N/A      1960      G   /usr/bin/gnome-shell               51MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14cd3f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: ModelNet10(3991)\n",
      "Test dataset shape:  ModelNet10(908)\n",
      "Data(pos=[1024, 3], y=[1], normal=[1024, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.transforms import SamplePoints\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_geometric.transforms import LinearTransformation\n",
    "from torch_geometric.transforms import GenerateMeshNormals\n",
    "from torch_geometric.transforms import NormalizeScale\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_scatter import scatter_mean\n",
    "import sys\n",
    "\n",
    "## NOTE:\n",
    "# Made it work with ModelNet10 (in folder \"data/ModelNet\") that has only 10 classes\n",
    "# Now I'll try to test it with ModelNet40 (\"data/ModelNet40\") with 40 classes\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Hyper parameters:\n",
    "num_points = 1024    # 1024 seems to be the limit...?\n",
    "batch_size = 32      # not yet used\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "modelnet_num = 10    # 10 or 40\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Choosing device:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "#                  NO LONGER USED but leave them here...\n",
    "F = [128, 512, 1024]  # Number of graph convolutional filters.\n",
    "K = [6, 5, 3]         # Polynomial orders.\n",
    "M = [512, 128, 10]    # Output dimensionality of fully connected layers.\n",
    "##########################################################################\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "## Data loading:\n",
    "# For ModelNet10 change root to \"data/ModelNet\"   -> 10 classes\n",
    "# For MpdelNet40 change root to \"data/ModelNet40\" -> 40 classes\n",
    "# Don't forget to change accordingly the output layer from the model...\n",
    "transforms = Compose([SamplePoints(num_points, include_normals=True), NormalizeScale()])\n",
    "\n",
    "root = \"data/ModelNet\"+str(modelnet_num)\n",
    "dataset_train = ModelNet(root=root, name=str(modelnet_num), train=True, transform=transforms)\n",
    "dataset_test = ModelNet(root=root, name=str(modelnet_num), train=False, transform=transforms)\n",
    "\n",
    "# Shuffle Data\n",
    "dataset_train = dataset_train.shuffle()\n",
    "dataset_test = dataset_test.shuffle()\n",
    "\n",
    "# Verification...\n",
    "print(f\"Train dataset shape: {dataset_train}\")\n",
    "print(f\"Test dataset shape:  {dataset_test}\")\n",
    "\n",
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5db4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(pos=[1024, 3], y=[1], normal=[1024, 3])\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982f90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch_geometric.utils as utils\n",
    "import torch_geometric.nn.conv as conv\n",
    "\n",
    "class GetGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates the weighted adjacency matrix 'W'\n",
    "        Taked directly from RGCNN\n",
    "        \"\"\"\n",
    "        super(GetGraph, self).__init__()\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        point_cloud_transpose = point_cloud.permute(0, 2, 1)\n",
    "        point_cloud_inner = torch.matmul(point_cloud, point_cloud_transpose)\n",
    "        point_cloud_inner = -2 * point_cloud_inner\n",
    "        point_cloud_square = torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True)\n",
    "        point_cloud_square_tranpose = point_cloud_square.permute(0, 2, 1)\n",
    "        adj_matrix = point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "        adj_matrix = torch.exp(-adj_matrix)\n",
    "        return adj_matrix\n",
    "\n",
    "\n",
    "class GetLaplacian(nn.Module):\n",
    "    def __init__(self, normalize=True):\n",
    "        \"\"\"\n",
    "        Computes the Graph Laplacian from a Weighted Graph\n",
    "        Taken directly from RGCNN - currently not used - might need to find alternatives in PyG for loss function\n",
    "        \"\"\"\n",
    "        super(GetLaplacian, self).__init__()\n",
    "        self.normalize = normalize\n",
    "\n",
    "        def diag(self, mat):\n",
    "        # input is batch x vertices\n",
    "            d = []\n",
    "            for vec in mat:\n",
    "                d.append(torch.diag(vec))\n",
    "            return torch.stack(d)\n",
    "\n",
    "    def forward(self, adj_matrix):\n",
    "        if self.normalize:\n",
    "            D = torch.sum(adj_matrix, dim=1)\n",
    "            eye = torch.ones_like(D)\n",
    "            eye = self.diag(eye)\n",
    "            D = 1 / torch.sqrt(D)\n",
    "            D = self.diag(D)\n",
    "            L = eye - torch.matmul(torch.matmul(D, adj_matrix), D)\n",
    "        else:\n",
    "            D = torch.sum(adj_matrix, dim=1)\n",
    "            D = torch.diag(D)\n",
    "            L = D - adj_matrix\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5583d500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = RGCNN_model(num_points, F, K, M)\\n\\nprint(\"Model\\'s state_dict:\")\\nfor param_tensor in model.state_dict():\\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\\n\\n# Print optimizer\\'s state_dict\\nprint(\"Optimizer\\'s state_dict:\")\\nfor var_name in optimizer.state_dict():\\n    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch_geometric.utils as utils\n",
    "import torch_geometric.nn.conv as conv\n",
    "import torch\n",
    "\n",
    "class RGCNN_model(nn.Module):\n",
    "    def __init__(self, vertice, F, K, M, regularization = 0, dropout = 0):\n",
    "        # verify the consistency w.r.t. the number of layers\n",
    "        assert len(F) == len(K)\n",
    "        super(RGCNN_model, self).__init__()\n",
    "        '''\n",
    "        F := List of Convolutional Layers dimensions\n",
    "        K := List of Chebyshev polynomial degrees\n",
    "        M := List of Fully Connected Layers dimenstions\n",
    "        \n",
    "        Currently the dimensions are 'hardcoded'\n",
    "        '''\n",
    "        self.F = F\n",
    "        self.K = K\n",
    "        self.M = M\n",
    "\n",
    "        self.vertice = vertice\n",
    "        self.regularization = regularization    # gamma from the paper: 10^-9\n",
    "        self.dropout = dropout\n",
    "        self.regularizers = []\n",
    "\n",
    "        # initialize the model layers\n",
    "        self.get_graph = GetGraph()\n",
    "        # self.get_laplacian = GetLaplacian(normalize=True)\n",
    "        self.pool = nn.MaxPool1d(self.vertice)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout)\n",
    "\n",
    "        ###################################################################\n",
    "        #                               CHANGE HERE\n",
    "        self.conv1 = conv.ChebConv(6, 128, 6)\n",
    "        self.conv2 = conv.ChebConv(128, 512, 5)\n",
    "        self.conv3 = conv.ChebConv(512, 1024, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, modelnet_num)\n",
    "        ###################################################################\n",
    "\n",
    "        '''\n",
    "        for i in range(len(F)):\n",
    "            if i == 0:\n",
    "                layer = ChebConv(Fin=3, K=K[i], Fout=F[i])\n",
    "            else:\n",
    "                layer = ChebConv(Fin=F[i-1], K=K[i], Fout=F[i])\n",
    "            setattr(self, 'gcn%d'%i, layer)\n",
    "        for i in range(len(M)):\n",
    "            if i==0:\n",
    "                layer = nn.Linear(F[-1], M[i])\n",
    "            else:\n",
    "                layer = nn.Linear(M[i-1], M[i])\n",
    "            setattr(self, 'fc%d'%i, layer)\n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.regularizers = []\n",
    "        # forward pass\n",
    "        W   = self.get_graph(x.detach())  # we don't want to compute gradients when building the graph\n",
    "        edge_index, edge_weight = utils.dense_to_sparse(W)\n",
    "        out = self.conv1(x, edge_index, edge_weight)\n",
    "        out = self.relu(out)\n",
    "        edge_index, edge_weight = utils.remove_self_loops(edge_index, edge_weight)\n",
    "        L_edge_index, L_edge_weight = torch_geometric.utils.get_laplacian(edge_index.detach(), edge_weight.detach(), normalization=\"sym\")\n",
    "        L = torch_geometric.utils.to_dense_adj(edge_index=L_edge_index, edge_attr=L_edge_weight)\n",
    "        self.regularizers.append(torch.linalg.norm(torch.matmul(torch.matmul(torch.Tensor.permute(out.detach(), [0, 2, 1]), L), out.detach())))\n",
    "\n",
    "        W   = self.get_graph(out.detach())\n",
    "        edge_index, edge_weight = utils.dense_to_sparse(W)\n",
    "        out = self.conv2(out, edge_index, edge_weight)\n",
    "        out = self.relu(out)\n",
    "        edge_index, edge_weight = utils.remove_self_loops(edge_index, edge_weight)\n",
    "        L_edge_index, L_edge_weight = torch_geometric.utils.get_laplacian(edge_index.detach(), edge_weight.detach(), normalization=\"sym\")\n",
    "        L = torch_geometric.utils.to_dense_adj(edge_index=L_edge_index, edge_attr=L_edge_weight)\n",
    "        self.regularizers.append(torch.linalg.norm(torch.matmul(torch.matmul(torch.Tensor.permute(out.detach(), [0, 2, 1]), L), out.detach())))\n",
    "\n",
    "        W   = self.get_graph(out.detach())\n",
    "        edge_index, edge_weight = utils.dense_to_sparse(W)\n",
    "        out = self.conv3(out, edge_index, edge_weight)\n",
    "        out = self.relu(out)\n",
    "        edge_index, edge_weight = utils.remove_self_loops(edge_index, edge_weight)\n",
    "        L_edge_index, L_edge_weight = torch_geometric.utils.get_laplacian(edge_index.detach(), edge_weight.detach(), normalization=\"sym\")\n",
    "        L = torch_geometric.utils.to_dense_adj(edge_index=L_edge_index, edge_attr=L_edge_weight)\n",
    "        self.regularizers.append(torch.linalg.norm(torch.matmul(torch.matmul(torch.Tensor.permute(out.detach(), [0, 2, 1]), L), out.detach())))\n",
    "\n",
    "        '''\n",
    "        for i in range(len(self.F)):\n",
    "            x = getattr(self, 'gcn%d'%i)(x, L)\n",
    "            print(x)\n",
    "            x = self.relu(x)\n",
    "        '''\n",
    "\n",
    "        out = out.permute(0, 2, 1) # Transpose\n",
    "        out = self.pool(out)\n",
    "        out.squeeze_(2)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        for param in self.fc1.parameters():\n",
    "            self.regularizers.append(torch.linalg.norm(param))\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        for param in self.fc1.parameters():\n",
    "            self.regularizers.append(torch.linalg.norm(param))\n",
    "        out = self.fc3(out)\n",
    "        for param in self.fc1.parameters():\n",
    "            self.regularizers.append(torch.linalg.norm(param))\n",
    "        '''\n",
    "        for i in range(len(self.M)):\n",
    "            x = getattr(self, \"fc%d\"%i)(x)\n",
    "        return x\n",
    "        '''\n",
    "\n",
    "        return out, self.regularizers\n",
    "'''\n",
    "model = RGCNN_model(num_points, F, K, M)\n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5521070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Sample: 0, Loss:2.3115339279174805 - Predicted class vs Real Cass: 6 <-> 5\n",
      "Epoch: 0, Sample: 100, Loss:2.277798891067505 - Predicted class vs Real Cass: 1 <-> 2\n",
      "Epoch: 0, Sample: 200, Loss:2.282564401626587 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 300, Loss:2.343125104904175 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 0, Sample: 400, Loss:2.158578395843506 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 500, Loss:2.209381580352783 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 0, Sample: 600, Loss:2.36948823928833 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 0, Sample: 700, Loss:2.0757012367248535 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 800, Loss:2.1975765228271484 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 0, Sample: 900, Loss:2.232785940170288 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 1000, Loss:2.4566924571990967 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 0, Sample: 1100, Loss:1.997214674949646 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 1200, Loss:2.2400872707366943 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 1300, Loss:2.7462916374206543 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 0, Sample: 1400, Loss:2.5753979682922363 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 0, Sample: 1500, Loss:2.1549935340881348 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 0, Sample: 1600, Loss:2.0159356594085693 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 0, Sample: 1700, Loss:1.9986768960952759 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 0, Sample: 1800, Loss:1.8763601779937744 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 1900, Loss:2.6462507247924805 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 0, Sample: 2000, Loss:2.343630790710449 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 0, Sample: 2100, Loss:2.881483316421509 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 0, Sample: 2200, Loss:2.267425298690796 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 0, Sample: 2300, Loss:1.8209826946258545 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 2400, Loss:2.2340965270996094 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 2500, Loss:2.2438406944274902 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 2600, Loss:2.351794719696045 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 0, Sample: 2700, Loss:1.787851333618164 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 2800, Loss:2.2249703407287598 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 2900, Loss:2.2277157306671143 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 3000, Loss:2.2986462116241455 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 0, Sample: 3100, Loss:2.677495241165161 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 0, Sample: 3200, Loss:1.7213771343231201 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 3300, Loss:2.360851526260376 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 0, Sample: 3400, Loss:2.0958497524261475 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 0, Sample: 3500, Loss:1.6715061664581299 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 0, Sample: 3600, Loss:2.3747780323028564 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 0, Sample: 3700, Loss:2.7463865280151367 - Predicted class vs Real Cass: 2 <-> 4\n",
      "Epoch: 0, Sample: 3800, Loss:2.186922311782837 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 0, Sample: 3900, Loss:2.1267776489257812 - Predicted class vs Real Cass: 2 <-> 1\n",
      "~~~~~~~~~ CORRECT: 0.21648709596592333 ~~~~~~~~~~~\n",
      "Epoch: 1, Sample: 0, Loss:2.1667041778564453 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 100, Loss:1.6360074281692505 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 200, Loss:2.1652259826660156 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 300, Loss:2.36088228225708 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 1, Sample: 400, Loss:1.5945615768432617 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 500, Loss:2.113632917404175 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 1, Sample: 600, Loss:2.793917179107666 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 1, Sample: 700, Loss:1.5767160654067993 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 800, Loss:2.1202754974365234 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 1, Sample: 900, Loss:2.1637516021728516 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 1000, Loss:2.837733268737793 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 1, Sample: 1100, Loss:1.5790681838989258 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 1200, Loss:2.1843435764312744 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 1300, Loss:3.3091015815734863 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 1, Sample: 1400, Loss:2.870028495788574 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 1, Sample: 1500, Loss:2.1057236194610596 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 1, Sample: 1600, Loss:1.8103208541870117 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 1, Sample: 1700, Loss:1.8020212650299072 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 1, Sample: 1800, Loss:1.5702934265136719 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 1900, Loss:2.901905059814453 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 1, Sample: 2000, Loss:2.4436893463134766 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 1, Sample: 2100, Loss:3.331167697906494 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 1, Sample: 2200, Loss:2.2969608306884766 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 1, Sample: 2300, Loss:1.5736387968063354 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 2400, Loss:2.2012603282928467 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 2500, Loss:2.212109327316284 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 2600, Loss:2.4336836338043213 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 1, Sample: 2700, Loss:1.5797064304351807 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 2800, Loss:2.194242000579834 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 2900, Loss:2.198235273361206 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 3000, Loss:2.320213794708252 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 1, Sample: 3100, Loss:2.8848867416381836 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 1, Sample: 3200, Loss:1.5550892353057861 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 3300, Loss:2.424147844314575 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 1, Sample: 3400, Loss:2.0685677528381348 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 1, Sample: 3500, Loss:1.525827169418335 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 1, Sample: 3600, Loss:2.4312081336975098 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 1, Sample: 3700, Loss:2.9153685569763184 - Predicted class vs Real Cass: 2 <-> 4\n",
      "Epoch: 1, Sample: 3800, Loss:2.1630048751831055 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 1, Sample: 3900, Loss:2.1030375957489014 - Predicted class vs Real Cass: 2 <-> 1\n",
      "~~~~~~~~~ CORRECT: 0.22275119017790027 ~~~~~~~~~~~\n",
      "Epoch: 2, Sample: 0, Loss:2.1438376903533936 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 100, Loss:1.524314284324646 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 200, Loss:2.142857551574707 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 300, Loss:2.3712573051452637 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 2, Sample: 400, Loss:1.4975945949554443 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 500, Loss:2.0923333168029785 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 2, Sample: 600, Loss:2.927351713180542 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 2, Sample: 700, Loss:1.4917395114898682 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 800, Loss:2.0998542308807373 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 2, Sample: 900, Loss:2.143510103225708 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 1000, Loss:2.9564056396484375 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 2, Sample: 1100, Loss:1.5085574388504028 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 1200, Loss:2.1651415824890137 - Predicted class vs Real Cass: 2 <-> 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Sample: 1300, Loss:3.5131664276123047 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 2, Sample: 1400, Loss:2.962411642074585 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 2, Sample: 1500, Loss:2.08744215965271 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 2, Sample: 1600, Loss:1.7643687725067139 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 2, Sample: 1700, Loss:1.7578134536743164 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 2, Sample: 1800, Loss:1.518985390663147 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 1900, Loss:2.9813661575317383 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 2, Sample: 2000, Loss:2.463087797164917 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 2, Sample: 2100, Loss:3.5000767707824707 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 2, Sample: 2200, Loss:2.2960364818573 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 2, Sample: 2300, Loss:1.5318857431411743 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 2400, Loss:2.1846256256103516 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 2500, Loss:2.195772647857666 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 2600, Loss:2.4478025436401367 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 2, Sample: 2700, Loss:1.5442934036254883 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 2800, Loss:2.1782076358795166 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 2900, Loss:2.1825103759765625 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 3000, Loss:2.3171463012695312 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 2, Sample: 3100, Loss:2.9489519596099854 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 2, Sample: 3200, Loss:1.5265167951583862 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 3300, Loss:2.4335074424743652 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 2, Sample: 3400, Loss:2.0546648502349854 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 2, Sample: 3500, Loss:1.5005398988723755 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 2, Sample: 3600, Loss:2.4389352798461914 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 2, Sample: 3700, Loss:2.9676620960235596 - Predicted class vs Real Cass: 2 <-> 4\n",
      "Epoch: 2, Sample: 3800, Loss:2.1495962142944336 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 2, Sample: 3900, Loss:2.0906054973602295 - Predicted class vs Real Cass: 2 <-> 1\n",
      "~~~~~~~~~ CORRECT: 0.22275119017790027 ~~~~~~~~~~~\n",
      "Epoch: 3, Sample: 0, Loss:2.130974054336548 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 100, Loss:1.5045909881591797 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 200, Loss:2.1304168701171875 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 300, Loss:2.366757869720459 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 3, Sample: 400, Loss:1.4803098440170288 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 500, Loss:2.0813136100769043 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 3, Sample: 600, Loss:2.9689626693725586 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 3, Sample: 700, Loss:1.4764071702957153 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 800, Loss:2.0894641876220703 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 3, Sample: 900, Loss:2.132650136947632 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 1000, Loss:2.99361515045166 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 3, Sample: 1100, Loss:1.4957042932510376 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 1200, Loss:2.1550326347351074 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 1300, Loss:3.5987062454223633 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 3, Sample: 1400, Loss:2.9909424781799316 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 3, Sample: 1500, Loss:2.078545331954956 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 3, Sample: 1600, Loss:1.7524075508117676 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 3, Sample: 1700, Loss:1.7463023662567139 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 3, Sample: 1800, Loss:1.5093817710876465 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 1900, Loss:3.0060362815856934 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 3, Sample: 2000, Loss:2.4638471603393555 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 3, Sample: 2100, Loss:3.5726747512817383 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 3, Sample: 2200, Loss:2.291320323944092 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 3, Sample: 2300, Loss:1.5238292217254639 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 2400, Loss:2.1766858100891113 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 2500, Loss:2.188028335571289 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 2600, Loss:2.4477810859680176 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 3, Sample: 2700, Loss:1.5372899770736694 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 2800, Loss:2.170797824859619 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 2900, Loss:2.175255537033081 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 3000, Loss:2.3126978874206543 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 3, Sample: 3100, Loss:2.969472646713257 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 3, Sample: 3200, Loss:1.520696759223938 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 3300, Loss:2.4327785968780518 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 3, Sample: 3400, Loss:2.0486817359924316 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 3, Sample: 3500, Loss:1.4952671527862549 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 3, Sample: 3600, Loss:2.437978982925415 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 3, Sample: 3700, Loss:2.984402894973755 - Predicted class vs Real Cass: 2 <-> 4\n",
      "Epoch: 3, Sample: 3800, Loss:2.143653154373169 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 3, Sample: 3900, Loss:2.0853352546691895 - Predicted class vs Real Cass: 2 <-> 1\n",
      "~~~~~~~~~ CORRECT: 0.22275119017790027 ~~~~~~~~~~~\n",
      "Epoch: 4, Sample: 0, Loss:2.1253104209899902 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 100, Loss:1.5003447532653809 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 200, Loss:2.1249895095825195 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 300, Loss:2.362983226776123 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 4, Sample: 400, Loss:1.4765199422836304 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 500, Loss:2.0767416954040527 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 4, Sample: 600, Loss:2.982382297515869 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 4, Sample: 700, Loss:1.4729883670806885 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 800, Loss:2.0851998329162598 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 4, Sample: 900, Loss:2.1280393600463867 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 1000, Loss:3.005645751953125 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 4, Sample: 1100, Loss:1.4927951097488403 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 1200, Loss:2.150783061981201 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 1300, Loss:3.6379666328430176 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 4, Sample: 1400, Loss:2.999880790710449 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 4, Sample: 1500, Loss:2.0749711990356445 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 4, Sample: 1600, Loss:1.7490378618240356 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 4, Sample: 1700, Loss:1.7430561780929565 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 4, Sample: 1800, Loss:1.5071269273757935 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 1900, Loss:3.0137722492218018 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 4, Sample: 2000, Loss:2.462393283843994 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 4, Sample: 2100, Loss:3.6064183712005615 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 4, Sample: 2200, Loss:2.2885329723358154 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 4, Sample: 2300, Loss:1.521868348121643 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 2400, Loss:2.173464775085449 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 2500, Loss:2.1848957538604736 - Predicted class vs Real Cass: 2 <-> 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Sample: 2600, Loss:2.4463491439819336 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 4, Sample: 2700, Loss:1.53553307056427 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 2800, Loss:2.1678216457366943 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 2900, Loss:2.1723437309265137 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 3000, Loss:2.3102619647979736 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 4, Sample: 3100, Loss:2.976120948791504 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 4, Sample: 3200, Loss:1.5191881656646729 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 3300, Loss:2.4313645362854004 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 4, Sample: 3400, Loss:2.0463712215423584 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 4, Sample: 3500, Loss:1.4938644170761108 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 4, Sample: 3600, Loss:2.4365806579589844 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 4, Sample: 3700, Loss:2.9897871017456055 - Predicted class vs Real Cass: 2 <-> 4\n",
      "Epoch: 4, Sample: 3800, Loss:2.1412997245788574 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 4, Sample: 3900, Loss:2.0833096504211426 - Predicted class vs Real Cass: 2 <-> 1\n",
      "~~~~~~~~~ CORRECT: 0.22275119017790027 ~~~~~~~~~~~\n",
      "Epoch: 5, Sample: 0, Loss:2.1230742931365967 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 100, Loss:1.4991811513900757 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 200, Loss:2.122851610183716 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 300, Loss:2.361078977584839 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 5, Sample: 400, Loss:1.4754620790481567 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 500, Loss:2.0749940872192383 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 5, Sample: 600, Loss:2.986708402633667 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 5, Sample: 700, Loss:1.4720227718353271 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 800, Loss:2.083573818206787 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 5, Sample: 900, Loss:2.126237630844116 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 1000, Loss:3.009521007537842 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 5, Sample: 1100, Loss:1.491962194442749 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 1200, Loss:2.1491281986236572 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 1300, Loss:3.656841516494751 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 5, Sample: 1400, Loss:3.0026183128356934 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 5, Sample: 1500, Loss:2.0736143589019775 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 5, Sample: 1600, Loss:1.747987151145935 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 5, Sample: 1700, Loss:1.7420413494110107 - Predicted class vs Real Cass: 2 <-> 7\n",
      "Epoch: 5, Sample: 1800, Loss:1.5064585208892822 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 1900, Loss:3.0161314010620117 - Predicted class vs Real Cass: 2 <-> 3\n",
      "Epoch: 5, Sample: 2000, Loss:2.461360454559326 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 5, Sample: 2100, Loss:3.6227471828460693 - Predicted class vs Real Cass: 2 <-> 0\n",
      "Epoch: 5, Sample: 2200, Loss:2.2872347831726074 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 5, Sample: 2300, Loss:1.5212692022323608 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 2400, Loss:2.1722161769866943 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 2500, Loss:2.183682441711426 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 2600, Loss:2.445406436920166 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 5, Sample: 2700, Loss:1.534982442855835 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 2800, Loss:2.166668176651001 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 2900, Loss:2.171215534210205 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 3000, Loss:2.3091495037078857 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 5, Sample: 3100, Loss:2.9782261848449707 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 5, Sample: 3200, Loss:1.5187039375305176 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 3300, Loss:2.430502414703369 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 5, Sample: 3400, Loss:2.045492649078369 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 5, Sample: 3500, Loss:1.4934033155441284 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 5, Sample: 3600, Loss:2.4357521533966064 - Predicted class vs Real Cass: 2 <-> 9\n",
      "Epoch: 5, Sample: 3700, Loss:2.991464614868164 - Predicted class vs Real Cass: 2 <-> 4\n",
      "Epoch: 5, Sample: 3800, Loss:2.1403889656066895 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 5, Sample: 3900, Loss:2.0825390815734863 - Predicted class vs Real Cass: 2 <-> 1\n",
      "~~~~~~~~~ CORRECT: 0.22275119017790027 ~~~~~~~~~~~\n",
      "Epoch: 6, Sample: 0, Loss:2.1222095489501953 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 6, Sample: 100, Loss:1.4987926483154297 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 6, Sample: 200, Loss:2.1220242977142334 - Predicted class vs Real Cass: 2 <-> 5\n",
      "Epoch: 6, Sample: 300, Loss:2.360231637954712 - Predicted class vs Real Cass: 2 <-> 8\n",
      "Epoch: 6, Sample: 400, Loss:1.4751043319702148 - Predicted class vs Real Cass: 2 <-> 2\n",
      "Epoch: 6, Sample: 500, Loss:2.074328899383545 - Predicted class vs Real Cass: 2 <-> 1\n",
      "Epoch: 6, Sample: 600, Loss:2.9880542755126953 - Predicted class vs Real Cass: 2 <-> 6\n",
      "Epoch: 6, Sample: 700, Loss:1.4716938734054565 - Predicted class vs Real Cass: 2 <-> 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-52aa2133cd39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# required by the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# to CUDA if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "directory = now.strftime(\"%d_%m_%y_%H:%M:%S\")\n",
    "parent_directory = \"/home/alex/Alex_pyt_geom/Models\"\n",
    "path = os.path.join(parent_directory, directory)\n",
    "os.mkdir(path)\n",
    "\n",
    "# Training\n",
    "\n",
    "# PATH = \"/home/alex/Alex_pyt_geom/Models/model\"\n",
    "model_number = 5                # Change this acording to the model you want to load\n",
    "model = RGCNN_model(num_points, F, K, M, dropout=1)\n",
    "\n",
    "# model.load_state_dict(torch.load(path + '/model' + str(model_number) + '.pt'))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "def get_loss(y, labels, regularization, regularizers):\n",
    "    cross_entropy_loss = loss(y, labels)\n",
    "    s = torch.sum(torch.as_tensor(regularizers))\n",
    "    regularization *= s\n",
    "    l = cross_entropy_loss + regularization\n",
    "    return l\n",
    "    \n",
    "correct_percentage_list = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    #dataset_train = dataset_train.shuffle()\n",
    "    correct = 0\n",
    "    for i, data in enumerate(dataset_train):\n",
    "        # make sure the gradients are empty\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Data preparation \n",
    "        pos = data.pos        # (num_points * 3)   \n",
    "        normals = data.normal # (num_points * 3)\n",
    "        x = torch.cat([pos, normals], dim=1)   # (num_points * 6)\n",
    "        x = x.unsqueeze(0)    # (1 * num_points * 6)     the first dimension may be used for batching?\n",
    "        x = x.type(torch.float32)  # other types of data may be unstable\n",
    "\n",
    "        y = data.y              # (1)\n",
    "        y = y.type(torch.long)  # required by the loss function\n",
    "        \n",
    "        x = x.to(device)      # to CUDA if available\n",
    "        y = y.to(device)\n",
    "     \n",
    "        # Forward pass\n",
    "        y_pred, regularizers = model(x)     # (1 * 40)\n",
    "        \n",
    "        class_pred = torch.argmax(y_pred.squeeze(0))  # (1)  \n",
    "        correct += int((class_pred == y).sum())       # to compute the accuracy for each epoch\n",
    "        \n",
    "\n",
    "        # loss and backward\n",
    "        ###################################################################################\n",
    "        #                           CrossEntropyLoss\n",
    "        # This WORKS but I am testing the other way...\n",
    "        # l = loss(y_pred, y)   # one value\n",
    "        # l.backward()          # update gradients\n",
    "        ###################################################################################\n",
    "       \n",
    "        l = get_loss(y_pred, y, regularization=1e-9, regularizers=regularizers)\n",
    "        l.backward()\n",
    "\n",
    "        # optimisation\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        if i%100==0:\n",
    "            print(f\"Epoch: {epoch}, Sample: {i}, Loss:{l} - Predicted class vs Real Cass: {class_pred} <-> {y.item()}\")\n",
    "            # print(torch.sum(torch.as_tensor(regularizers)))\n",
    "        if epoch%5==0:\n",
    "            torch.save(model.state_dict(), path + '/model' + str(epoch) + '.pt')\n",
    "    print(f\"~~~~~~~~~ CORRECT: {correct / len(dataset_train)} ~~~~~~~~~~~\")\n",
    "    correct_percentage_list.append(correct / len(dataset_train))\n",
    "print(correct_percentage_list)\n",
    "\n",
    "torch.save(model.state_dict(), \"/home/alex/Alex_pyt_geom/Models/final_model.pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in dataset_test:\n",
    "        pos = data.pos        # (num_points * 3)   \n",
    "        normals = data.normal # (num_points * 3)\n",
    "        x = torch.cat([pos, normals], dim=1)   # (num_points * 6)\n",
    "        x = x.unsqueeze(0)    # (1 * num_points * 6)     the first dimension may be used for batching?\n",
    "        x = x.type(torch.float32)  # other types of data may be unstable\n",
    "\n",
    "        y = data.y              # (1)\n",
    "        y = y.type(torch.long)  # required by the loss function\n",
    "        \n",
    "        x = x.to(device)      # to CUDA if available\n",
    "        y = y.to(device)\n",
    "     \n",
    "        # Forward pass\n",
    "        y_pred, _ = model(x)     # (1 * 40)\n",
    "\n",
    "        class_pred = torch.argmax(y_pred)\n",
    "        correct += int((class_pred == y).sum())\n",
    "\n",
    "    print(f\"Correct percentage : {correct / len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct_percentage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85919859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in dataset_test:\n",
    "#         pos = data.pos        # (num_points * 3)   \n",
    "#         normals = data.normal # (num_points * 3)\n",
    "#         x = torch.cat([pos, normals], dim=1)   # (num_points * 6)\n",
    "#         x = x.unsqueeze(0)    # (1 * num_points * 6)     the first dimension may be used for batching?\n",
    "#         x = x.type(torch.float32)  # other types of data may be unstable\n",
    "\n",
    "#         y = data.y              # (1)\n",
    "#         y = y.type(torch.long)  # required by the loss function\n",
    "        \n",
    "#         x = x.to(device)      # to CUDA if available\n",
    "#         y = y.to(device)\n",
    "     \n",
    "#         # Forward pass\n",
    "#         y_pred = model(x)     # (1 * 40)\n",
    "\n",
    "#         class_pred = torch.argmax(y_pred)\n",
    "#         # print(\"Pred: \", class_pred.item(), \"Real: \" , y.item())\n",
    "#         correct += int((class_pred == y).sum())\n",
    "\n",
    "#     print(f\"Correct percentage : {correct / len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f9783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in dataset_test:\n",
    "        \n",
    "#         pos = data.pos        # (num_points * 3)   \n",
    "#         normals = data.normal # (num_points * 3)\n",
    "#         x = torch.cat([pos, normals], dim=1)   # (num_points * 6)\n",
    "#         x = x.unsqueeze(0)    # (1 * num_points * 6)     the first dimension may be used for batching?\n",
    "#         x = x.type(torch.float32)  # other types of data may be unstable\n",
    "\n",
    "#         y = data.y              # (1)\n",
    "#         y = y.type(torch.long)  # required by the loss function\n",
    "        \n",
    "#         x = x.to(device)      # to CUDA if available\n",
    "#         y = y.to(device)\n",
    "     \n",
    "#         # Forward pass\n",
    "#         y_pred, _ = model(x)     # (1 * 40)\n",
    "\n",
    "#         class_pred = torch.argmax(y_pred)\n",
    "#         print(\"Pred: \", class_pred.item(), \"Real: \" , y.item())\n",
    "#         correct += int((class_pred == y_pred).sum())\n",
    "\n",
    "#     print(f\"Correct percentage : {correct / len(dataset_test)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     x = torch.rand([1, 1024, 6])*5+3\n",
    "#     x = x.to(device)\n",
    "#     y = model(x)\n",
    "#     y_class = torch.argmax(y)\n",
    "#     print(y)\n",
    "#     print(y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348f533",
   "metadata": {},
   "source": [
    "WORK IN PROGRESS!!! \n",
    "Trying to create batches from data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e229a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "length = len(dataset_train)\n",
    "batch_size = 32\n",
    "iterations = np.ceil(length/batch_size)\n",
    "iterations = iterations.astype(int)\n",
    "batched_data = torch.empty([125, 1024, 6])\n",
    "print(dataset_train)\n",
    "print(dataset_train[0])\n",
    "aux = Data()\n",
    "for i in range(iterations):\n",
    "    ob = dataset_train[i:i+batch_size]\n",
    "    pos=torch.empty([0, 3])\n",
    "    y = torch.empty([0])\n",
    "    normal = torch.empty([0, 3])\n",
    "    for data in ob:\n",
    "        pos = torch.cat([pos, data.pos])\n",
    "        y = torch.cat([y, data.y])\n",
    "        normal = torch.cat([normal, data.normal])\n",
    "    batch_data[i] = Data(pos=pos, y=y, normal=normal)\n",
    "print(pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pos.shape)\n",
    "#print(pos)\n",
    "print(len(batch_data))\n",
    "Batched_data = torch.empty([125, 1024, 6])\n",
    "BATCHED_DATA = []\n",
    "for i in range(125):\n",
    "    # print(batch_data[i].pos)\n",
    "    pos = torch.empty([32, 1024, 3])\n",
    "    y = torch.empty([32, 1])\n",
    "    normal = torch.empty([32, 1024, 3])\n",
    "    for i in range(batch_size):\n",
    "        pos[i] = batch_data[i].pos[num_points*i:num_points*i+1024]\n",
    "        y[i] = batch_data[i].y[i]\n",
    "        normal[i] = batch_data[i].normal[num_points*i:num_points*i+1024]\n",
    "    BATCH = Data(pos=pos, y=y, normal=normal)\n",
    "    BATCHED_DATA.append(BATCH)\n",
    "    # Batched_data[i] = Data(pos=pos, y=y, normal=normal)\n",
    "print(pos.shape)\n",
    "print(normal.shape)\n",
    "print(y.shape)\n",
    "print(len(BATCHED_DATA))\n",
    "for data in BATCHED_DATA:\n",
    "    print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
