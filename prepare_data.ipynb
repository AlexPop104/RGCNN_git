{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ModelNet dataset. Support ModelNet40, ModelNet10, XYZ and normal channels. Up to 10000 points.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "class ModelNetDataset():\n",
    "    def __init__(self, root, batch_size = 32, npoints = 1024, split='train', normalize=True, normal_channel=True, modelnet10=False, cache_size=15000, shuffle=None):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.npoints = npoints\n",
    "        self.normalize = normalize\n",
    "        if modelnet10:\n",
    "            self.catfile = os.path.join(self.root, 'modelnet10_shape_names.txt')\n",
    "        else:\n",
    "            self.catfile = os.path.join(self.root, 'modelnet40_shape_names.txt')\n",
    "        self.cat = [line.rstrip() for line in open(self.catfile)]\n",
    "        self.classes = dict(zip(self.cat, range(len(self.cat))))  \n",
    "        self.normal_channel = normal_channel\n",
    "        \n",
    "        shape_ids = {}\n",
    "        if modelnet10:\n",
    "            shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet10_train.txt'))] \n",
    "            shape_ids['test']= [line.rstrip() for line in open(os.path.join(self.root, 'modelnet10_test.txt'))]\n",
    "        else:\n",
    "            shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_train.txt'))] \n",
    "            shape_ids['test']= [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_test.txt'))]\n",
    "        assert(split=='train' or split=='test')\n",
    "        shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]\n",
    "        # list of (shape_name, shape_txt_file_path) tuple\n",
    "        self.datapath = [(shape_names[i], os.path.join(self.root, shape_names[i], shape_ids[split][i])+'.txt') for i in range(len(shape_ids[split]))]\n",
    "\n",
    "        self.cache_size = cache_size # how many data points to cache in memory\n",
    "        self.cache = {} # from index to (point_set, cls) tuple\n",
    "\n",
    "        if shuffle is None:\n",
    "            if split == 'train': self.shuffle = True\n",
    "            else: self.shuffle = False\n",
    "        else:\n",
    "            self.shuffle = shuffle\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _augment_batch_data(self, batch_data):\n",
    "        if self.normal_channel:\n",
    "            rotated_data = provider.rotate_point_cloud_with_normal(batch_data)\n",
    "            rotated_data = provider.rotate_perturbation_point_cloud_with_normal(rotated_data)\n",
    "        else:\n",
    "            rotated_data = provider.rotate_point_cloud(batch_data)\n",
    "            rotated_data = provider.rotate_perturbation_point_cloud(rotated_data)\n",
    "    \n",
    "        jittered_data = provider.random_scale_point_cloud(rotated_data[:,:,0:3])\n",
    "        jittered_data = provider.shift_point_cloud(jittered_data)\n",
    "        jittered_data = provider.jitter_point_cloud(jittered_data)\n",
    "        rotated_data[:,:,0:3] = jittered_data\n",
    "        return provider.shuffle_points(rotated_data)\n",
    "\n",
    "\n",
    "    def _get_item(self, index): \n",
    "        if index in self.cache:\n",
    "            point_set, cls = self.cache[index]\n",
    "        else:\n",
    "            fn = self.datapath[index]\n",
    "            cls = self.classes[self.datapath[index][0]]\n",
    "            cls = np.array([cls]).astype(np.int32)\n",
    "            point_set = np.loadtxt(fn[1],delimiter=',').astype(np.float32)\n",
    "            # Take the first npoints\n",
    "            point_set = point_set[0:self.npoints,:]\n",
    "            if self.normalize:\n",
    "                point_set[:,0:3] = pc_normalize(point_set[:,0:3])\n",
    "            if not self.normal_channel:\n",
    "                point_set = point_set[:,0:3]\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[index] = (point_set, cls)\n",
    "        return point_set, cls\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self._get_item(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def num_channel(self):\n",
    "        if self.normal_channel:\n",
    "            return 6\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.idxs = np.arange(0, len(self.datapath))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.num_batches = (len(self.datapath)+self.batch_size-1) // self.batch_size\n",
    "        self.batch_idx = 0\n",
    "\n",
    "    def has_next_batch(self):\n",
    "        return self.batch_idx < self.num_batches\n",
    "\n",
    "    def next_batch(self, augment=False):\n",
    "        ''' returned dimension may be smaller than self.batch_size '''\n",
    "        start_idx = self.batch_idx * self.batch_size\n",
    "        end_idx = min((self.batch_idx+1) * self.batch_size, len(self.datapath))\n",
    "        bsize = end_idx - start_idx\n",
    "        batch_data = np.zeros((bsize, self.npoints, self.num_channel()))\n",
    "        batch_label = np.zeros((bsize), dtype=np.int32)\n",
    "        for i in range(bsize):\n",
    "            ps,cls = self._get_item(self.idxs[i+start_idx])\n",
    "            batch_data[i] = ps\n",
    "            batch_label[i] = cls\n",
    "        self.batch_idx += 1\n",
    "        if augment: batch_data = self._augment_batch_data(batch_data)\n",
    "        return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/alex/Alex_documents/RGCNN/data/modelnet40_normal_resampled/modelnet40_shape_names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5195/870422440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelNetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/alex/Alex_documents/RGCNN/data/modelnet40_normal_resampled/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormal_channel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5195/1765498001.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, batch_size, npoints, split, normalize, normal_channel, modelnet10, cache_size, shuffle)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modelnet40_shape_names.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/alex/Alex_documents/RGCNN/data/modelnet40_normal_resampled/modelnet40_shape_names.txt'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "d = ModelNetDataset(\"/home/alex/Alex_documents/RGCNN/data/modelnet40_normal_resampled/\",split=\"train\",normal_channel=True)\n",
    "train_data = np.zeros((len(d),1024,6))\n",
    "train_label = np.zeros((len(d)))\n",
    "print(train_data.shape)\n",
    "for i in tqdm(range(len(d))):\n",
    "    pc, label = d[i]\n",
    "    train_data[i] = pc\n",
    "    train_label[i] = label\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2468 [00:00<04:15,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2468, 1024, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2468/2468 [02:37<00:00, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2468, 1024, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d = ModelNetDataset(\"/home/victor/workspace/thesis_ws/data/modelnet40_normal_resampled/\",split=\"test\",normal_channel=True)\n",
    "test_data = np.zeros((len(d),1024,6))\n",
    "test_label = np.zeros((len(d)))\n",
    "print(test_data.shape)\n",
    "for i in tqdm(range(len(d))):\n",
    "    pc,label = d[i]\n",
    "    test_data[i] = pc\n",
    "    test_label[i] = label\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cls_data_train.npy\",train_data)\n",
    "np.save(\"cls_label_train.npy\",train_label)\n",
    "np.save(\"cls_data_test.npy\",test_data)\n",
    "np.save(\"cls_label_test.npy\",test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/victor/.local/lib/python3.6/site-packages (4.61.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing segmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-efa718b62c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMouseCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'show3d'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monmouse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypeslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'render_balls_so'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshowpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_gt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mwaittime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshowrot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmagnifyBlue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreezerot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbackground\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalizecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mballradius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BASE_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" Original Author: Haoqiang Fan \"\"\"\n",
    "import numpy as np\n",
    "import ctypes as ct\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "showsz=800\n",
    "mousex,mousey=0.5,0.5\n",
    "zoom=1.0\n",
    "changed=True\n",
    "def onmouse(*args):\n",
    "    global mousex,mousey,changed\n",
    "    y=args[1]\n",
    "    x=args[2]\n",
    "    mousex=x/float(showsz)\n",
    "    mousey=y/float(showsz)\n",
    "    changed=True\n",
    "cv2.namedWindow('show3d')\n",
    "cv2.moveWindow('show3d',0,0)\n",
    "cv2.setMouseCallback('show3d',onmouse)\n",
    "\n",
    "dll=np.ctypeslib.load_library(os.path.join(BASE_DIR, 'render_balls_so'),'.')\n",
    "\n",
    "def showpoints(xyz,c_gt=None, c_pred = None ,waittime=0,showrot=False,magnifyBlue=0,freezerot=False,background=(0,0,0),normalizecolor=True,ballradius=10):\n",
    "    global showsz,mousex,mousey,zoom,changed\n",
    "    xyz=xyz-xyz.mean(axis=0)\n",
    "    radius=((xyz**2).sum(axis=-1)**0.5).max()\n",
    "    xyz/=(radius*2.2)/showsz\n",
    "    if c_gt is None:\n",
    "        c0=np.zeros((len(xyz),),dtype='float32')+255\n",
    "        c1=np.zeros((len(xyz),),dtype='float32')+255\n",
    "        c2=np.zeros((len(xyz),),dtype='float32')+255\n",
    "    else:\n",
    "        c0=c_gt[:,0]\n",
    "        c1=c_gt[:,1]\n",
    "        c2=c_gt[:,2]\n",
    "\n",
    "\n",
    "    if normalizecolor:\n",
    "        c0/=(c0.max()+1e-14)/255.0\n",
    "        c1/=(c1.max()+1e-14)/255.0\n",
    "        c2/=(c2.max()+1e-14)/255.0\n",
    "\n",
    "\n",
    "    c0=np.require(c0,'float32','C')\n",
    "    c1=np.require(c1,'float32','C')\n",
    "    c2=np.require(c2,'float32','C')\n",
    "\n",
    "    show=np.zeros((showsz,showsz,3),dtype='uint8')\n",
    "    def render():\n",
    "        rotmat=np.eye(3)\n",
    "        if not freezerot:\n",
    "            xangle=(mousey-0.5)*np.pi*1.2\n",
    "        else:\n",
    "            xangle=0\n",
    "        rotmat=rotmat.dot(np.array([\n",
    "            [1.0,0.0,0.0],\n",
    "            [0.0,np.cos(xangle),-np.sin(xangle)],\n",
    "            [0.0,np.sin(xangle),np.cos(xangle)],\n",
    "            ]))\n",
    "        if not freezerot:\n",
    "            yangle=(mousex-0.5)*np.pi*1.2\n",
    "        else:\n",
    "            yangle=0\n",
    "        rotmat=rotmat.dot(np.array([\n",
    "            [np.cos(yangle),0.0,-np.sin(yangle)],\n",
    "            [0.0,1.0,0.0],\n",
    "            [np.sin(yangle),0.0,np.cos(yangle)],\n",
    "            ]))\n",
    "        rotmat*=zoom\n",
    "        nxyz=xyz.dot(rotmat)+[showsz/2,showsz/2,0]\n",
    "\n",
    "        ixyz=nxyz.astype('int32')\n",
    "        show[:]=background\n",
    "        dll.render_ball(\n",
    "            ct.c_int(show.shape[0]),\n",
    "            ct.c_int(show.shape[1]),\n",
    "            show.ctypes.data_as(ct.c_void_p),\n",
    "            ct.c_int(ixyz.shape[0]),\n",
    "            ixyz.ctypes.data_as(ct.c_void_p),\n",
    "            c0.ctypes.data_as(ct.c_void_p),\n",
    "            c1.ctypes.data_as(ct.c_void_p),\n",
    "            c2.ctypes.data_as(ct.c_void_p),\n",
    "            ct.c_int(ballradius)\n",
    "        )\n",
    "\n",
    "        if magnifyBlue>0:\n",
    "            show[:,:,0]=np.maximum(show[:,:,0],np.roll(show[:,:,0],1,axis=0))\n",
    "            if magnifyBlue>=2:\n",
    "                show[:,:,0]=np.maximum(show[:,:,0],np.roll(show[:,:,0],-1,axis=0))\n",
    "            show[:,:,0]=np.maximum(show[:,:,0],np.roll(show[:,:,0],1,axis=1))\n",
    "            if magnifyBlue>=2:\n",
    "                show[:,:,0]=np.maximum(show[:,:,0],np.roll(show[:,:,0],-1,axis=1))\n",
    "        if showrot:\n",
    "            cv2.putText(show,'xangle %d'%(int(xangle/np.pi*180)),(30,showsz-30),0,0.5,cv2.cv.CV_RGB(255,0,0))\n",
    "            cv2.putText(show,'yangle %d'%(int(yangle/np.pi*180)),(30,showsz-50),0,0.5,cv2.cv.CV_RGB(255,0,0))\n",
    "            cv2.putText(show,'zoom %d%%'%(int(zoom*100)),(30,showsz-70),0,0.5,cv2.cv.CV_RGB(255,0,0))\n",
    "    changed=True\n",
    "    while True:\n",
    "        if changed:\n",
    "            render()\n",
    "            changed=False\n",
    "        cv2.imshow('show3d',show)\n",
    "        if waittime==0:\n",
    "            cmd=cv2.waitKey(10)%256\n",
    "        else:\n",
    "            cmd=cv2.waitKey(waittime)%256\n",
    "        if cmd==ord('q'):\n",
    "            break\n",
    "        elif cmd==ord('Q'):\n",
    "            sys.exit(0)\n",
    "\n",
    "        if cmd==ord('t') or cmd == ord('p'):\n",
    "            if cmd == ord('t'):\n",
    "                if c_gt is None:\n",
    "                    c0=np.zeros((len(xyz),),dtype='float32')+255\n",
    "                    c1=np.zeros((len(xyz),),dtype='float32')+255\n",
    "                    c2=np.zeros((len(xyz),),dtype='float32')+255\n",
    "                else:\n",
    "                    c0=c_gt[:,0]\n",
    "                    c1=c_gt[:,1]\n",
    "                    c2=c_gt[:,2]\n",
    "            else:\n",
    "                if c_pred is None:\n",
    "                    c0=np.zeros((len(xyz),),dtype='float32')+255\n",
    "                    c1=np.zeros((len(xyz),),dtype='float32')+255\n",
    "                    c2=np.zeros((len(xyz),),dtype='float32')+255\n",
    "                else:\n",
    "                    c0=c_pred[:,0]\n",
    "                    c1=c_pred[:,1]\n",
    "                    c2=c_pred[:,2]\n",
    "            if normalizecolor:\n",
    "                c0/=(c0.max()+1e-14)/255.0\n",
    "                c1/=(c1.max()+1e-14)/255.0\n",
    "                c2/=(c2.max()+1e-14)/255.0\n",
    "            c0=np.require(c0,'float32','C')\n",
    "            c1=np.require(c1,'float32','C')\n",
    "            c2=np.require(c2,'float32','C')\n",
    "            changed = True\n",
    "\n",
    "\n",
    "\n",
    "        if cmd==ord('n'):\n",
    "            zoom*=1.1\n",
    "            changed=True\n",
    "        elif cmd==ord('m'):\n",
    "            zoom/=1.1\n",
    "            changed=True\n",
    "        elif cmd==ord('r'):\n",
    "            zoom=1.0\n",
    "            changed=True\n",
    "        elif cmd==ord('s'):\n",
    "            cv2.imwrite('show3d.png',show)\n",
    "        if waittime!=0:\n",
    "            break\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "class PartNormalDataset():\n",
    "    def __init__(self, root, npoints = 2500, classification = False, split='train', normalize=True, return_cls_label = False):\n",
    "        self.npoints = npoints\n",
    "        self.root = root\n",
    "        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n",
    "        self.cat = {}\n",
    "        \n",
    "        self.classification = classification\n",
    "        self.normalize = normalize\n",
    "        self.return_cls_label = return_cls_label\n",
    "        \n",
    "        with open(self.catfile, 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.cat[ls[0]] = ls[1]\n",
    "        self.cat = {k:v for k,v in self.cat.items()}\n",
    "        #print(self.cat)\n",
    "            \n",
    "        self.meta = {}\n",
    "        with open(os.path.join(self.root, 'train_test_split', 'shuffled_train_file_list.json'), 'r') as f:\n",
    "            train_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n",
    "        with open(os.path.join(self.root, 'train_test_split', 'shuffled_val_file_list.json'), 'r') as f:\n",
    "            val_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n",
    "        with open(os.path.join(self.root, 'train_test_split', 'shuffled_test_file_list.json'), 'r') as f:\n",
    "            test_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n",
    "        for item in self.cat:\n",
    "            #print('category', item)\n",
    "            self.meta[item] = []\n",
    "            dir_point = os.path.join(self.root, self.cat[item])\n",
    "            fns = sorted(os.listdir(dir_point))\n",
    "            #print(fns[0][0:-4])\n",
    "            if split=='trainval':\n",
    "                fns = [fn for fn in fns if ((fn[0:-4] in train_ids) or (fn[0:-4] in val_ids))]\n",
    "            elif split=='train':\n",
    "                fns = [fn for fn in fns if fn[0:-4] in train_ids]\n",
    "            elif split=='val':\n",
    "                fns = [fn for fn in fns if fn[0:-4] in val_ids]\n",
    "            elif split=='test':\n",
    "                fns = [fn for fn in fns if fn[0:-4] in test_ids]\n",
    "            else:\n",
    "                print('Unknown split: %s. Exiting..'%(split))\n",
    "                exit(-1)\n",
    "                \n",
    "            #print(os.path.basename(fns))\n",
    "            for fn in fns:\n",
    "                token = (os.path.splitext(os.path.basename(fn))[0]) \n",
    "                self.meta[item].append(os.path.join(dir_point, token + '.txt'))\n",
    "        \n",
    "        self.datapath = []\n",
    "        for item in self.cat:\n",
    "            for fn in self.meta[item]:\n",
    "                self.datapath.append((item, fn))\n",
    "            \n",
    "         \n",
    "        self.classes = dict(zip(self.cat, range(len(self.cat))))  \n",
    "        # Mapping from category ('Chair') to a list of int [10,11,12,13] as segmentation labels\n",
    "        self.seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43], 'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46], 'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27], 'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n",
    "\n",
    "        for cat in sorted(self.seg_classes.keys()):\n",
    "            print(cat, self.seg_classes[cat])\n",
    "        \n",
    "        self.cache = {} # from index to (point_set, cls, seg) tuple\n",
    "        self.cache_size = 20000\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if index in self.cache:\n",
    "            point_set, normal, seg, cls = self.cache[index]\n",
    "        else:\n",
    "            fn = self.datapath[index]\n",
    "            cat = self.datapath[index][0]\n",
    "            cls = self.classes[cat]\n",
    "            cls = np.array([cls]).astype(np.int32)\n",
    "            data = np.loadtxt(fn[1]).astype(np.float32)\n",
    "            point_set = data[:,0:3]\n",
    "            if self.normalize:\n",
    "                point_set = pc_normalize(point_set)\n",
    "            normal = data[:,3:6]\n",
    "            seg = data[:,-1].astype(np.int32)\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[index] = (point_set, normal, seg, cls)\n",
    "                \n",
    "        \n",
    "        choice = np.random.choice(len(seg), self.npoints, replace=True)\n",
    "        #resample\n",
    "        point_set = point_set[choice, :]\n",
    "        seg = seg[choice]\n",
    "        normal = normal[choice,:]\n",
    "        if self.classification:\n",
    "            return point_set, normal, cls\n",
    "        else:\n",
    "            if self.return_cls_label:\n",
    "                return point_set, normal, seg, cls\n",
    "            else:\n",
    "                return point_set, normal, seg\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane [0, 1, 2, 3]\n",
      "Bag [4, 5]\n",
      "Cap [6, 7]\n",
      "Car [8, 9, 10, 11]\n",
      "Chair [12, 13, 14, 15]\n",
      "Earphone [16, 17, 18]\n",
      "Guitar [19, 20, 21]\n",
      "Knife [22, 23]\n",
      "Lamp [24, 25, 26, 27]\n",
      "Laptop [28, 29]\n",
      "Motorbike [30, 31, 32, 33, 34, 35]\n",
      "Mug [36, 37]\n",
      "Pistol [38, 39, 40]\n",
      "Rocket [41, 42, 43]\n",
      "Skateboard [44, 45, 46]\n",
      "Table [47, 48, 49]\n",
      "13998\n",
      "('Airplane', '../data/shapenetcore_partanno_segmentation_benchmark_v0_normal/02691156/3fe365251b54087af0478431b5ad57db.txt')\n",
      "3 0\n",
      "(1024, 3) (1024,) (1024, 3)\n",
      "[[ 0.09738774  0.00546969 -0.8964166 ]\n",
      " [ 0.16769218 -0.00677618 -0.41418064]\n",
      " [ 0.01516543 -0.08585501  0.74770147]\n",
      " ...\n",
      " [ 0.2265981  -0.01117705  0.32111856]\n",
      " [-0.5533657   0.10272599  0.0414042 ]\n",
      " [ 0.19956425  0.01574748  0.09921676]]\n",
      "[[ 0.0325    0.9995    0.003013]\n",
      " [-0.09137   0.9957    0.01664 ]\n",
      " [-0.1217    0.7157    0.6878  ]\n",
      " ...\n",
      " [ 0.01265   0.9997    0.02266 ]\n",
      " [ 0.01154   0.9373    0.3483  ]\n",
      " [-0.01631   0.09668   0.9952  ]]\n",
      "Airplane [0, 1, 2, 3]\n",
      "Bag [4, 5]\n",
      "Cap [6, 7]\n",
      "Car [8, 9, 10, 11]\n",
      "Chair [12, 13, 14, 15]\n",
      "Earphone [16, 17, 18]\n",
      "Guitar [19, 20, 21]\n",
      "Knife [22, 23]\n",
      "Lamp [24, 25, 26, 27]\n",
      "Laptop [28, 29]\n",
      "Motorbike [30, 31, 32, 33, 34, 35]\n",
      "Mug [36, 37]\n",
      "Pistol [38, 39, 40]\n",
      "Rocket [41, 42, 43]\n",
      "Skateboard [44, 45, 46]\n",
      "Table [47, 48, 49]\n",
      "12137\n",
      "(2500, 3) <class 'numpy.ndarray'> (1,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    d = PartNormalDataset(root = '../data/shapenetcore_partanno_segmentation_benchmark_v0_normal', split='trainval', npoints=1024)\n",
    "    print(len(d))\n",
    "\n",
    "    i = 500\n",
    "    ps, normal, seg = d[i]\n",
    "    print (d.datapath[i])\n",
    "    print (np.max(seg), np.min(seg))\n",
    "    print (ps.shape, seg.shape, normal.shape)\n",
    "    print (ps)\n",
    "    print (normal)\n",
    "    \n",
    "    sys.path.append('../utils')\n",
    "    \n",
    "    # showpoints(ps, normal+1, ballradius=8)\n",
    "\n",
    "    d = PartNormalDataset(root = '../data/shapenetcore_partanno_segmentation_benchmark_v0_normal', classification = True)\n",
    "    print(len(d))\n",
    "    ps, normal, cls = d[0]\n",
    "    print(ps.shape, type(ps), cls.shape,type(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/12137 [00:00<03:18, 61.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane [0, 1, 2, 3]\n",
      "Bag [4, 5]\n",
      "Cap [6, 7]\n",
      "Car [8, 9, 10, 11]\n",
      "Chair [12, 13, 14, 15]\n",
      "Earphone [16, 17, 18]\n",
      "Guitar [19, 20, 21]\n",
      "Knife [22, 23]\n",
      "Lamp [24, 25, 26, 27]\n",
      "Laptop [28, 29]\n",
      "Motorbike [30, 31, 32, 33, 34, 35]\n",
      "Mug [36, 37]\n",
      "Pistol [38, 39, 40]\n",
      "Rocket [41, 42, 43]\n",
      "Skateboard [44, 45, 46]\n",
      "Table [47, 48, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12137/12137 [03:26<00:00, 58.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: (13998, 2048, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_pos = np.zeros([len(d), 2048, 3])\n",
    "train_seg = np.zeros([len(d), 2048])\n",
    "train_norm = np.zeros([len(d), 2048, 3])\n",
    "d = PartNormalDataset(root = '../data/shapenetcore_partanno_segmentation_benchmark_v0_normal', split='train', npoints=2048)\n",
    "\n",
    "for i in tqdm(range(len(d))):\n",
    "    ps, normal, seg = d[i]\n",
    "    train_pos[i] = ps\n",
    "    train_seg[i] = seg\n",
    "    train_norm[i] = normal\n",
    "print(f\"Pos: {train_pos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"seg_train_pos.npy\", train_pos)\n",
    "np.save(\"seg_train_seg.npy\", train_seg)\n",
    "np.save(\"seg_train_norm.npy\", train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1870 [00:00<00:34, 54.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane [0, 1, 2, 3]\n",
      "Bag [4, 5]\n",
      "Cap [6, 7]\n",
      "Car [8, 9, 10, 11]\n",
      "Chair [12, 13, 14, 15]\n",
      "Earphone [16, 17, 18]\n",
      "Guitar [19, 20, 21]\n",
      "Knife [22, 23]\n",
      "Lamp [24, 25, 26, 27]\n",
      "Laptop [28, 29]\n",
      "Motorbike [30, 31, 32, 33, 34, 35]\n",
      "Mug [36, 37]\n",
      "Pistol [38, 39, 40]\n",
      "Rocket [41, 42, 43]\n",
      "Skateboard [44, 45, 46]\n",
      "Table [47, 48, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1870/1870 [00:32<00:00, 57.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: (12137, 2048, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "val_pos = np.zeros([len(d), 2048, 3])\n",
    "val_seg = np.zeros([len(d), 2048])\n",
    "val_norm = np.zeros([len(d), 2048, 3])\n",
    "d = PartNormalDataset(root = '../data/shapenetcore_partanno_segmentation_benchmark_v0_normal', split='val', npoints=2048)\n",
    "\n",
    "for i in tqdm(range(len(d))):\n",
    "    ps, normal, seg = d[i]\n",
    "    val_pos[i] = ps\n",
    "    val_seg[i] = seg\n",
    "    val_norm[i] = normal\n",
    "print(f\"Pos: {val_pos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"seg_val_pos.npy\", val_pos)\n",
    "np.save(\"seg_val_seg.npy\", val_seg)\n",
    "np.save(\"seg_val_norm.npy\", val_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2874 [00:00<00:50, 56.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane [0, 1, 2, 3]\n",
      "Bag [4, 5]\n",
      "Cap [6, 7]\n",
      "Car [8, 9, 10, 11]\n",
      "Chair [12, 13, 14, 15]\n",
      "Earphone [16, 17, 18]\n",
      "Guitar [19, 20, 21]\n",
      "Knife [22, 23]\n",
      "Lamp [24, 25, 26, 27]\n",
      "Laptop [28, 29]\n",
      "Motorbike [30, 31, 32, 33, 34, 35]\n",
      "Mug [36, 37]\n",
      "Pistol [38, 39, 40]\n",
      "Rocket [41, 42, 43]\n",
      "Skateboard [44, 45, 46]\n",
      "Table [47, 48, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2874/2874 [00:48<00:00, 58.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: (2874, 2048, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "d = PartNormalDataset(root = '../data/shapenetcore_partanno_segmentation_benchmark_v0_normal', split='test', npoints=2048)\n",
    "test_pos = np.zeros([len(d), 2048, 3])\n",
    "test_seg = np.zeros([len(d), 2048])\n",
    "test_norm = np.zeros([len(d), 2048, 3])\n",
    "\n",
    "for i in tqdm(range(len(d))):\n",
    "    ps, normal, seg = d[i]\n",
    "    test_pos[i] = ps\n",
    "    test_seg[i] = seg\n",
    "    test_norm[i] = normal\n",
    "print(f\"Pos: {test_pos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"seg_test_pos.npy\", test_pos)\n",
    "np.save(\"seg_test_seg.npy\", test_seg)\n",
    "np.save(\"seg_test_norm.npy\", test_norm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92d26938485ea6bea67bf2e2bb4662b06e39d7cc20cef26f940ca706eab939d3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
